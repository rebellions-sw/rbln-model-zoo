# Triton Inference Server with vLLM enabled
Supplementary guide for the tutorial on serving LLMs with the Triton Inference Server.

# Files
- material
  - Contains the source files extracted from the document page, used in this tutorial.
- check_env.sh
  - A Bash script to verify all necessary preparations for running this tutorial properly.
- setup.sh
  - A setup script for preparing the required files and configurations for this tutorial.

# Result
You can check the `output` directory, which contains all the necessary preparations for running the tutorial.

# References
[Triton Inference Server with vLLM enabled](https://docs.rbln.ai/tutorial/advanced/llm_serving_vllm.html#triton-inference-server-with-vllm-enabled)
