# Llama2-7B with Continous Batching 
Supplementary guide for the tutorial on serving LLMs with the Triton Inference Server.

# Files
- material
  - Contains the source files extracted from the document page, used in this tutorial.
- check_env.sh
  - A Bash script to verify all necessary preparations for running this tutorial properly.
- setup.sh
  - A setup script for preparing the required files and configurations for this tutorial.

# Result
You can check the `output` directory, which contains all the necessary preparations for running the tutorial.

# References
[Llama2-7B with Continous Batching](https://docs.rbln.ai/software/model_serving/nvidia_triton_inference_server/tutorial/llama2-7B_continous_batching.html)
