# RBLN Cosmos-Transfer1

## Prerequisites

### Submodule Update

Before proceeding, make sure to update the submodules:

```bash
git submodule update --init ./cosmos-transfer1
```

### Download Checkpoints

After updating the submodules, you need to download the checkpoints.

Follow the instructions below to download checkpoints.


1. Move to the updated submodule (Cosmos-Transfer1).

```bash
cd cosmos-transfer1
```

2. Generate a [Hugging Face](https://huggingface.co/settings/tokens) access token. Set the access token to 'Read' permission (default is 'Fine-grained').

3. Log in to Hugging Face with the access token:

```bash
huggingface-cli login
```

4. Accept the [Llama-Guard-3-8B terms](https://huggingface.co/meta-llama/Llama-Guard-3-8B).

5. Download the Cosmos model weights from [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-transfer1-67c9d328196453be6e568d3e):

> [!note]
> Before download, you should accept **NVIDIA Open Model License Agreement** for the model collection.
> Please make sure that you accept the agreement for the models below.
> - [nvidia/Cosmos-Guardrail1](https://huggingface.co/nvidia/Cosmos-Guardrail1)
> - [nvidia/Cosmos-Tokenize1-CV8x8x8-720p](https://huggingface.co/nvidia/Cosmos-Tokenize1-CV8x8x8-720p)

```bash
PYTHONPATH=$(pwd) python scripts/download_checkpoints.py --output_dir ../ckpt --model 7b
```

Please verify that `--output_dir` points to the directory where you want to save the checkpoints.


Please refer to the following document for detailed instructions: [Download Checkpoints Guide](https://github.com/nvidia-cosmos/cosmos-transfer1/blob/main/examples/inference_cosmos_transfer1_7b.md#download-checkpoints)

## Quick Test

After downloading checkpoints, you can run examples with the below scripts.

We assume that you already download the original checkpoints to `./ckpt`.

Please refer to the following contents for detailed information.


### Distilled ControlNet

```bash
python compile.py transfer --checkpoint_dir ./ckpt --controlnet_specs assets/single_test.json --rbln_dir ./rbln_ckpt_distil --use_distilled
python inference.py transfer --checkpoint_dir ./ckpt --controlnet_specs assets/single_test.json --rbln_dir ./rbln_ckpt_distil --use_distilled --video_save_folder ./outputs/distil --num_steps 1
```

### Single ControlNet

```bash
python compile.py transfer --checkpoint_dir ./ckpt --controlnet_specs assets/single_test.json --rbln_dir ./rbln_ckpt
python inference.py transfer --checkpoint_dir ./ckpt --controlnet_specs assets/single_test.json --rbln_dir ./rbln_ckpt --video_save_folder ./outputs/single
```

### Regional Prompt Example

```bash
python compile.py transfer --checkpoint_dir ./ckpt --controlnet_specs assets/regional_prompt_test.json --rbln_dir ./rbln_ckpt_region --use_regional_prompts --num_regions 3
python inference.py transfer --checkpoint_dir ./ckpt --controlnet_specs assets/regional_prompt_test.json --rbln_dir ./rbln_ckpt_region --video_save_folder ./outputs/region
```

### Multiview Example

```bash
python compile.py transfer_multiview --checkpoint_dir ./ckpt --controlnet_specs assets/sample_av_hdmap_multiview_spec.json --rbln_dir ./rbln_ckpt_multiview
python inference.py transfer_multiview --checkpoint_dir ./ckpt --controlnet_specs assets/sample_av_hdmap_multiview_spec.json --rbln_dir ./rbln_ckpt_multiview --view_condition_video assets/sample_av_mv_input_rgb.mp4 --video_save_folder ./outputs/multiview --guidance 3 --num_steps 30
```


## Example JSON file

```json
{
    "prompt": "The video shows a sequence of images where two women are facing each other, both wearing floral headscarves and traditional-style dresses. They appear to be engaged in a conversation, with their facial expressions and body language suggesting a friendly or intimate interaction. In the background, there is a man dressed in a military uniform, standing with his back to the camera, seemingly observing the women. The setting looks like a room with posters on the wall, and the overall atmosphere is casual and domestic.",
    "input_video_path" : "assets/example1.mp4",
    "vis": {
        "control_weight": 0.2
    },
    "edge": {
        "control_weight": 0.2
    },
    "depth": {
        "input_control": "assets/example1_depth.mp4",
        "control_weight": 0.2
    },
    "seg": {
        "control_weight": 0.2
    },
    "keypoint": {
        "control_weight": 0.2
    }
}
```

The above example is `assets/multi_test.json`.

- `prompt`: The input prompt that describes the video.

- `input_video_path`: Path to a video (.mp4) file that you want to reconstruct.

- `vis`, `edge`, `depth`, `seg`, `keypoint`: ControlNets to use. If you want to use only a subset of these 5 ControlNets, remove the other(s) that you don't want to use.

- `control_weight`: The strength indicating how much a specific ControlNet affects the output video.

- `input_control`: Path to a video (.mp4) file used as the input of a ControlNet. If not provided, the ControlNet input is generated by Preprocessors.


## Compile Models

Example scripts are provided in the `scripts` directory.

> [!note]
> If you saved checkpoints to another path, please set the environment variable `CHECKPOINT_DIR` before running scripts, as follows:
>
> ```bash
> export CHECKPOINT_DIR=$PATH_TO_CKPT
> ```


### NPU Resource Optimization Settings

While generating videos, a lot of RBLN devices are needed.

If you don't have enough devices, we recommend configurations that uses fewer devices.

To do so, we provide NPU resource optimization settings that try to use less number of devices.


#### Multi ControlNets

You can use up to 5 ControlNets at once.

We provides a unified script that compiles all ControlNets along with other modules.

```bash
bash ./scripts/compile_multi.sh
```

#### Single ControlNet

You can use a subset of the compiled ControlNets that you compiled the above at inference time.

However, if you want to compile modules with only a single ControlNet, run the script right below.

(Please update `assets/single_test.json` to compile the desired ControlNet.)

```bash
bash ./scripts/compile_single.sh
```

#### Regional Prompt Example

We support regional prompts, and you should compile models for regional prompt examples.

```
bash ./scripts/compile_region.sh
```

#### Distilled ControlNet

With the distilled ControlNet, we can generate videos in a single diffusion step, thereby the inference time is reduced dramatically.

To compile a distilled ControlNet and other necessary modules, run:

```bash
bash ./scripts/compile_distil.sh
```

> [!note]
> Currently, NVIDIA provides the Edge ControlNet only for the distilled version.

#### 4K Upscaler

With 4K upscaler, you can upscale 720p-resolution videos to 4K-resolution videos.

To compile modules for 4K upscaler, run:

```bash
bash ./scripts/compile_4kupscaler.sh
```

#### Sample-AV

There are two more modalities that specialized for autonomous vehicle applications.

To compile modules for Sample-AV, run:

```bash
bash ./scripts/compile_av.sh
```

#### Sample-AV Single to Multiview

With this model, you can generate multi-view video with a single video.

To compile modules for Sample-AV Single to Multiview, run:

```bash
bash ./scripts/compile_multiview.sh
```

#### Sample-AV Single to Multiview Video2World

With a multi-view video generated with single to multiview model, you can extend the video with this model.

To compile modules for Sample-AV Single to Multiview Video2World, run:

```bash
bash ./scripts/compile_multiview_lvg.sh
```


### Latency Optimization Setting

If you have enough devices to run multiple ControlNets, we recommend the configuration that generates videos faster than NPU resource optimization setting.

These files have `_perf` postfix in the file name; other options are the same as in the NPU resource optimization setting.



## Inference Models

Inference scripts correspond to compile scripts.

If you compile models with `scripts/compile_XXX.sh` file, run the corresponding `scripts/inferece_XXX.sh` file to generate videos with the compiled models.
